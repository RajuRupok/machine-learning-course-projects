{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a582eb",
   "metadata": {},
   "source": [
    "# MultiClass AdaBoost with ID3 as Base Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425dbfe6",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the implementation of the AdaBoost algorithm using the ID3 decision tree as the base learner. \n",
    "The dataset used is the Letter Recognition dataset, where the goal is to classify 26 capital letters based on numerical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978abf5e",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3709924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16000 samples\n",
      "Test set size: 4000 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "columns = [\n",
    "    'letter', 'x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', \n",
    "    'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', \n",
    "    'x-ege', 'xegvy', 'y-ege', 'yegvx'\n",
    "]\n",
    "data = pd.read_csv('letter-recognition.data', header=None, names=columns)\n",
    "\n",
    "# Map letters to numeric classes for easier processing\n",
    "data['letter'] = data['letter'].apply(lambda x: ord(x) - ord('A'))\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.iloc[:, 1:]\n",
    "y = data['letter']\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ede9a",
   "metadata": {},
   "source": [
    "## Step 2: Implement the ID3 Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde5a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ID3DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
    "\n",
    "    def _information_gain(self, X_column, y, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "        left_idx = X_column <= threshold\n",
    "        right_idx = X_column > threshold\n",
    "        if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
    "            return 0\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_idx]), len(y[right_idx])\n",
    "        e_left, e_right = self._entropy(y[left_idx]), self._entropy(y[right_idx])\n",
    "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            X_column = X[:, feature_index]\n",
    "            thresholds = np.unique(X_column)\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(X_column, y, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        if depth == self.max_depth or n_labels == 1 or n_samples == 0:\n",
    "            return np.bincount(y).argmax()\n",
    "        feature_index, threshold = self._best_split(X, y)\n",
    "        if feature_index is None:\n",
    "            return np.bincount(y).argmax()\n",
    "        left_idxs = X[:, feature_index] <= threshold\n",
    "        right_idxs = X[:, feature_index] > threshold\n",
    "        left_subtree = self._build_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "        return (feature_index, threshold, left_subtree, right_subtree)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, 0)\n",
    "\n",
    "    def _traverse_tree(self, x, tree):\n",
    "        if not isinstance(tree, tuple):\n",
    "            return tree\n",
    "        feature_index, threshold, left, right = tree\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self._traverse_tree(x, left)\n",
    "        return self._traverse_tree(x, right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307cd2a",
   "metadata": {},
   "source": [
    "## Step 3: Implement the AdaBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b98f366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiClassAdaBoost:\n",
    "    def __init__(self, base_learner_class, n_estimators=50):\n",
    "        self.base_learner_class = base_learner_class\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        self.classes = np.unique(y)\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = self.base_learner_class(max_depth=5)\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "            incorrect = (y_pred != y).astype(int)\n",
    "            error = np.dot(weights, incorrect) / np.sum(weights)\n",
    "            if error >= 0.5 or error == 0:\n",
    "                break\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            weights = weights * np.exp(alpha * incorrect)\n",
    "            weights /= np.sum(weights)\n",
    "            self.models.append(model)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_votes = np.zeros((X.shape[0], len(self.classes)))\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            predictions = model.predict(X)\n",
    "            for i, pred in enumerate(predictions):\n",
    "                class_index = np.where(self.classes == pred)[0][0]\n",
    "                class_votes[i, class_index] += alpha\n",
    "        return self.classes[np.argmax(class_votes, axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e75682",
   "metadata": {},
   "source": [
    "## Step 4: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cacdb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 3.95%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      1.00      0.08       158\n",
      "           1       0.00      0.00      0.00       153\n",
      "           2       0.00      0.00      0.00       147\n",
      "           3       0.00      0.00      0.00       161\n",
      "           4       0.00      0.00      0.00       154\n",
      "           5       0.00      0.00      0.00       155\n",
      "           6       0.00      0.00      0.00       155\n",
      "           7       0.00      0.00      0.00       147\n",
      "           8       0.00      0.00      0.00       151\n",
      "           9       0.00      0.00      0.00       149\n",
      "          10       0.00      0.00      0.00       148\n",
      "          11       0.00      0.00      0.00       152\n",
      "          12       0.00      0.00      0.00       158\n",
      "          13       0.00      0.00      0.00       157\n",
      "          14       0.00      0.00      0.00       150\n",
      "          15       0.00      0.00      0.00       161\n",
      "          16       0.00      0.00      0.00       157\n",
      "          17       0.00      0.00      0.00       151\n",
      "          18       0.00      0.00      0.00       150\n",
      "          19       0.00      0.00      0.00       159\n",
      "          20       0.00      0.00      0.00       163\n",
      "          21       0.00      0.00      0.00       153\n",
      "          22       0.00      0.00      0.00       150\n",
      "          23       0.00      0.00      0.00       157\n",
      "          24       0.00      0.00      0.00       157\n",
      "          25       0.00      0.00      0.00       147\n",
      "\n",
      "    accuracy                           0.04      4000\n",
      "   macro avg       0.00      0.04      0.00      4000\n",
      "weighted avg       0.00      0.04      0.00      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadma\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\sadma\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\sadma\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train MultiClass AdaBoost\n",
    "multiclass_adaboost = MultiClassAdaBoost(base_learner_class=ID3DecisionTree, n_estimators=10)\n",
    "multiclass_adaboost.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = multiclass_adaboost.predict(X_test.values)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
